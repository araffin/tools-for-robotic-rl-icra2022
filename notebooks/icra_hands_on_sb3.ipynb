{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araffin/tools-for-robotic-rl-icra2022/blob/main/notebooks/icra_hands_on_sb3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Gym/Stable Baselines3 Getting Started - ICRA 2022\n",
        "\n",
        "Github repo: https://github.com/araffin/tools-for-robotic-rl-icra2022\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "You will also learn how to define a gym wrapper to customise the training.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWskDE2c9WoN",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYdv2ygjLaFL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oexj67yWN5_k",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Optional: install SB3 contrib to have access to additional algorithms\n",
        "!pip install sb3-contrib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNkrgcI6Z1"
      },
      "source": [
        "# Part I: Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the Gym interface\n",
        "\n",
        "An environment that follows the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) is quite simple to use.\n",
        "It provides to this user mainly three methods:\n",
        "- `reset()` called at the beginning of an episode, it returns an observation\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
        "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)\n",
        "\n",
        "Below you can find an example of a custom environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "owrHOGhlMGLE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, NamedTuple, Tuple, Union\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "GymObs = Union[Tuple, Dict, np.ndarray, int]\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Minimal custom environment to demonstrate the Gym interface.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(14,))\n",
        "    self.action_space = gym.spaces.Box(low=-1, high=1, shape=(6,))\n",
        "\n",
        "  def reset(self) -> GymObs:\n",
        "    \"\"\"\n",
        "    Called at the beginning of an episode.\n",
        "    :return: the first observation of the episode\n",
        "    \"\"\"\n",
        "    return self.observation_space.sample()\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    \"\"\"\n",
        "    Step into the environment.\n",
        "    :return: A tuple containing the new observation, the reward signal, \n",
        "      whether the episode is over and additional informations.\n",
        "    \"\"\"\n",
        "    obs = self.observation_space.sample()\n",
        "    reward = 1.0\n",
        "    done = False\n",
        "    # Whether the termination was due to timeout or not\n",
        "    info = {\"TimeLimit.truncated\": False}\n",
        "    return obs, reward, done, info\n",
        "\n",
        "env = CustomEnv()\n",
        "# Check your custom environment\n",
        "# this will print warnings and throw errors if needed\n",
        "check_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://www.gymlibrary.ml/).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym document is still a work in progress.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BIedd7Pz9sOs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R7tKaBFrTR0a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EcsXmYRMON9W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Algorithms from the contrib repo\n",
        "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "from sb3_contrib import QRDQN, TQC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO(\"MlpPolicy\", env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ROUJr675TT01",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use Pendulum environment, a classic control problem.\n",
        "\n",
        "\"The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\"\n",
        "\n",
        "Pendulum environment: [https://www.gymlibrary.ml/environments/classic_control/pendulum/](https://www.gymlibrary.ml/environments/classic_control/pendulum/)\n",
        "\n",
        "\n",
        "![Pendulum-v1](https://huggingface.co/sb3/ppo-Pendulum-v1/resolve/main/pendulum.gif)\n",
        "\n",
        "We chose the MlpPolicy because the observation of the Pendulum task is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://sb3-contrib.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create the gym Env\n",
        "env_id = \"Pendulum-v1\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the RL agent\n",
        "# Here we are using tuned hyperparameters\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    gamma=0.98,\n",
        "    use_sde=True,\n",
        "    sde_sample_freq=4,\n",
        "    learning_rate=1e-3,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K2YzpGDwQ4"
      },
      "source": [
        "### Using the model to predict actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRfb_f7uD-H9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rF_xsDEADvv_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve first observation\n",
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZgrRoM23EU9Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict the action to take given the observation\n",
        "action, _ = model.predict(obs, deterministic=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1wRSvPrEjUa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# We are using continuous actions, therefore `action` is a numpy array\n",
        "assert env.action_space.contains(action)\n",
        "\n",
        "print(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtEtKqKEvFC"
      },
      "source": [
        "Step in the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJ-yv5MEllY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "obs, reward, done, info = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTNOOlOdE1lH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"obs_shape={obs.shape}, reward={reward}, done? {done}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dd6hT873ImE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Reset the env at the end of an episode\n",
        "if done:\n",
        "    obs = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSeXppUFNLg"
      },
      "source": [
        "### Exercise (10 minutes): write the function to evaluate the agent\n",
        "\n",
        "This function will be used to evaluate the performance of an RL agent.\n",
        "Thanks to Stable Baselines3 interface, it will work with any SB3 algorithms and any Gym environment.\n",
        "\n",
        "See docstring of the function for what is expected as input/output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf7Ktha2FE3B",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    env: gym.Env,\n",
        "    n_eval_episodes: int = 100,\n",
        "    deterministic: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate an RL agent for `n_eval_episodes`.\n",
        "\n",
        "    :param model: the RL Agent\n",
        "    :param env: the gym Environment\n",
        "    :param n_eval_episodes: number of episodes to evaluate it\n",
        "    :param deterministic: Whether to use deterministic or stochastic actions\n",
        "    :return: Mean episodic reward for the last `n_eval_episodes`\n",
        "     (Mean over episodes of the cumulative episodic reward)\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "    # TODO: run `n_eval_episodes` episodes in the Gym env\n",
        "    # using the RL agent and keep track of the total reward\n",
        "    # collected for each episode (aka cumulative reward or episode return).\n",
        "    # Finally, compute the mean and print it\n",
        "    total_reward_per_episode = []\n",
        "    for _ in range(n_eval_episodes):\n",
        "        done = False\n",
        "        cumulative_reward = 0.0\n",
        "        obs = env.reset()\n",
        "        # Loop until the episode terminates\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=deterministic)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            # Update cumulative reward\n",
        "            cumulative_reward += reward\n",
        "            if done:\n",
        "                total_reward_per_episode.append(cumulative_reward)\n",
        "    \n",
        "    # Print some infos\n",
        "    mean_episode_reward = np.mean(total_reward_per_episode)\n",
        "    std_reward = np.std(total_reward_per_episode)\n",
        "    print(f\"Mean episode reward = {mean_episode_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3SahgtZH2TX"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGmJK4oHUeT",
        "outputId": "f3b1fbf7-d6c3-470c-e6ae-8e61a3fe4be6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 21.45 Num episodes: 100\n"
          ]
        }
      ],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, env, n_eval_episodes=20, deterministic=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines already provides you with that helper (the actual implementation is a little more advanced):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z6K9YImYJEx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lHSorUyH8I8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# The Monitor wrapper allows to keep track of the training reward and other infos (useful for plotting)\n",
        "env = Monitor(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oPTHjxyZSOL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Seed to compare to previous implementation\n",
        "# env.reset(seed=42) with gym 0.23\n",
        "env.seed(42)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Train the agent for 50 000 steps\n",
        "model.learn(total_timesteps=50_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyfQxD5z26J",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLzXxO8VMD6N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you can learn more about those wrappers in our Documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trag9dQpOIhx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iATu7AiyMQW2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "record_video('Pendulum-v1', model, video_length=500, prefix='ppo-pendulum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "show_videos('videos', prefix='ppo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5WHto5hNaZN"
      },
      "source": [
        "### [Optional] Exercise (5 minutes): Save, Load The Model and that the loading was correct\n",
        "\n",
        "Save the model and then load it.\n",
        "\n",
        "Don't forget to check that loading went well: the model must predict the same actions given the same  observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCcS6wJ6Nurd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sample observations using the environment observation space\n",
        "observations = np.array([env.observation_space.sample() for _ in range(10)])\n",
        "\n",
        "# Predict actions on those observations using trained model\n",
        "action_before_saving, _ = model.predict(observations, deterministic=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWCUZSD7NZ_Y",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"ppo_pendulum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2M1S_-V2gMi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Delete the model (to demonstrate loading)\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ne1MlIiNpee",
        "outputId": "0350fd76-db7b-4943-c7e1-9bfa9a4c7ddd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ppo_cartpole.zip\n"
          ]
        }
      ],
      "source": [
        "!ls *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55G-6nJLNrPk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = PPO.load(\"ppo_pendulum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIWII5YxNtrI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict actions on the observations with the loaded model\n",
        "action_after_loading, _ = model.predict(observations, deterministic=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m06ipphZN_0E",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Check that the predictions are the same\n",
        "assert np.allclose(action_before_saving, action_after_loading), \"Somethng went wrong in the loading\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm4RncfCJFyP"
      },
      "source": [
        "# Part II: Gym Wrappers\n",
        "\n",
        "\n",
        "In this part, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYo0C0TQSL3c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env:  Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, infos = self.env.step(action)\n",
        "    return obs, reward, done, infos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RkTLshPMGq"
      },
      "source": [
        "### [Optional] Exercise (7 minutes): limit the episode length\n",
        "\n",
        "In this exercise, the goal is to create a Gym wrapper that will limit the maximum number of steps per episode (timeout).\n",
        "\n",
        "\n",
        "It will also pass a `timeout` signal in the info dict to tell the agent that the termination was due to reaching the limits.\n",
        "\n",
        "Termination due to timeout must be handled separately (see [Time Limit in RL paper](https://arxiv.org/abs/1712.00378)), you can also take a look at [Issue #284](https://github.com/DLR-RM/stable-baselines3/issues/284) and [Issue #633](https://github.com/DLR-RM/stable-baselines3/issues/633)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_ypIxgPLGL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Limit the maximum number of steps per episode.\n",
        "\n",
        "  :param env: Gym environment that will be wrapped\n",
        "  :param max_steps: Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env, max_steps: int = 100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # YOUR CODE HERE\n",
        "    # Counter of steps per episode\n",
        "    self.n_steps = 0\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "  \n",
        "  def reset(self) -> GymObs:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: reset the counter and reset the env\n",
        "    self.n_steps = 0\n",
        "    obs = self.env.reset()\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs\n",
        "\n",
        "  def step(self, action: Union[int, np.ndarray]) -> Tuple[GymObs, float, bool, Dict]:\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: \n",
        "    # 1. Step into the env\n",
        "    # 2. Increment the episode counter\n",
        "    # 3. Overwrite the done signal when time limit is reached \n",
        "    # (optional) 4. update the info dict (add a \"episode_timeout\" key)\n",
        "    # when the episode was stopped due to timelimit\n",
        "    obs, reward, done, infos = self.env.step(action)\n",
        "    self.n_steps += 1\n",
        "    if self.n_steps >= self.max_steps:\n",
        "      # Note: in gym, the key is called \"TimeLimit.truncated\"\n",
        "      # Termination due to this wrapper only (timeout)\n",
        "      infos[\"episode_timeout\"] = not done\n",
        "      done = True\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    return obs, reward, done, infos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQptLSuPB3A"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szZ43D5PVB07",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cencka9iVg9V",
        "outputId": "01ddfdb8-be4c-4df9-e052-d4a54666419e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode length: 100 steps, info dict: {'episode_timeout': True}\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, infos = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(f\"Episode length: {n_steps} steps, info dict: {infos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "What we have seen in this notebook:\n",
        "- SB3 101\n",
        "- Gym wrappers to modify the env\n",
        "- more complete tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "- longer hands-on session: https://www.youtube.com/watch?v=Ikngt0_DXJg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-gqIPXqV7zZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "icra22_hands_on_sb3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
